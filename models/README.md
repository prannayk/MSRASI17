## Next steps in training

1. batch normalization of embeddings
2. embedding penalization
3. use nltk tokenizer
4. use nltk stemmer
5. run on brown corpus
6. run on twitter samples
7. run on twitter data
8. run on 50k nepal dataset
9. run on italy dataset
10. save trained model